{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16db3167",
   "metadata": {},
   "source": [
    "\n",
    "# Fine‑tune Whisper Large‑v3 Turbo on Urdu Common Voice 17.0  \n",
    "\n",
    "End-to-end fine-tuning of Whisper-v3-turbo on Urdu with TensorBoard experiment tracking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98441ef-f21d-4d62-8ebd-25e8a3aacc2e",
   "metadata": {},
   "source": [
    "## 0. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b8383b-5d3c-49cd-be5b-4f7d198f0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers accelerate datasets evaluate peft librosa bitsandbytes \\\n",
    "             huggingface_hub tensorboard jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cea9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from huggingface_hub import login, HfApi\n",
    "from datasets import load_dataset, load_from_disk, Audio\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DefaultFlowCallback,\n",
    ")\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load as load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdacee-4b63-4a9d-81ab-3acbda1e09c1",
   "metadata": {},
   "source": [
    "## 1. Configuration & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872c6be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79a1d419b030>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED         = 420\n",
    "HF_TOKEN     = os.environ.get(\"HF_TOKEN\")           # your HF token\n",
    "HF_USERNAME  = \"kingabzpro\"                         # replace as needed\n",
    "MODEL_ID     = \"openai/whisper-large-v3-turbo\"\n",
    "LANG_ID      = \"ur\"\n",
    "SAMPLING_RATE= 16_000\n",
    "CACHE_DIR    = \"./cached_cv_urdu\"\n",
    "PUSH_MODEL_ID = f\"{HF_USERNAME}/whisper-large-v3-turbo-urdu\"\n",
    "\n",
    "assert HF_TOKEN, \"Please set HF_TOKEN env var to your Hugging Face token\"\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca7345-8473-49e5-848f-d29634f4d2af",
   "metadata": {},
   "source": [
    "## 2. Helpers: text & audio cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b125b1-bbc4-4c56-92a8-305b8712b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_urdu(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[“”«»„”…—–\\[\\]\\(\\)]', '', text)\n",
    "    digits_map = str.maketrans(\"۰۱۲۳۴۵۶۷۸۹\", \"0123456789\")\n",
    "    return text.translate(digits_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d34b3-1fc6-467b-895c-86cc0997ea84",
   "metadata": {},
   "source": [
    "## 3. Login & Load Model + Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac72507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8afb4d2fb534c3f92f2cc25f1325fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6cd7c45dbe4be880c4496aa0270dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2510938e8b0449d98764c11bc90f868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6217191307c8407ca3db0a1a66920f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e022943daf594b2c981ab811e04d2a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94be3f5125b34e11983cbb01f71b6b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dc16b8974a4a38938b3bf4e811190a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1779e53906384c23a8a21cde4dd74cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7533687551184a49890e1df69ed76268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0fa8e4bea14bd5b07c548420339848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba65d8d32224b64961496dc3cadabc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login(HF_TOKEN)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID, language=LANG_ID, task=\"transcribe\"\n",
    ")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    MODEL_ID, ignore_mismatched_sizes=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.generation_config.language = \"ur\"\n",
    "model.generation_config.task = \"transcribe\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba73e15-de69-4c2d-9b0b-91b11a13231b",
   "metadata": {},
   "source": [
    "## 4. Preprocess + Cache Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efd27ac-2976-460e-acaf-3afc6b0ccd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(batch):\n",
    "    # batch[\"audio\"] is a list of dictionaries because batched=True.\n",
    "    # We create a list of all the audio arrays from the batch.\n",
    "    audio_arrays = [x[\"array\"] for x in batch[\"audio\"]]\n",
    "    \n",
    "    # We can assume the sampling rate is the same for all items in the batch.\n",
    "    sampling_rate = batch[\"audio\"][0][\"sampling_rate\"]\n",
    "\n",
    "    input_feats = processor.feature_extractor(\n",
    "        audio_arrays, sampling_rate=sampling_rate, return_tensors=\"np\"\n",
    "    ).input_features\n",
    "\n",
    "    cleaned = [normalize_urdu(t) for t in batch[\"sentence\"]]\n",
    "    labels  = processor.tokenizer(\n",
    "        cleaned,\n",
    "        truncation=True,\n",
    "        max_length=model.config.max_length, # Uncomment if 'model' is defined\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\"\n",
    "    ).input_ids\n",
    "\n",
    "    # The function should return a dictionary\n",
    "    return {\"input_features\": input_feats, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551f8316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400c8cace3494690b03ed1d32bd94ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, DatasetDict, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "if not os.path.isdir(CACHE_DIR):\n",
    "    # 1. load both splits into a DatasetDict\n",
    "    dataset = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_17_0\",\n",
    "        LANG_ID,\n",
    "        split={\"train\": \"train+validation\", \"validation\": \"test[:600]\"},\n",
    "        cache_dir=\"./hf_cache\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # 2. drop unwanted columns\n",
    "    dataset = dataset.remove_columns(\n",
    "        [col for col in raw[\"train\"].column_names if col not in (\"audio\",\"sentence\")]\n",
    "    )\n",
    "    print(raw)\n",
    "    \n",
    "    # 3. \n",
    "    # Cast audio & preprocess\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "    dataset = dataset.map(\n",
    "        prepare_example,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        desc=\"Pre‑processing\",\n",
    "        batched=True,\n",
    "        batch_size=125,\n",
    "        load_from_cache_file=True,\n",
    "    )\n",
    "\n",
    "    # 4. save all splits\n",
    "    DatasetDict(dataset).save_to_disk(CACHE_DIR)\n",
    "\n",
    "# 5. load and set torch format\n",
    "dataset = load_from_disk(CACHE_DIR)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_features\",\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12587b8e-58dd-44c0-b6d7-26d7d48d02d0",
   "metadata": {},
   "source": [
    "## 5. Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0626a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # 1. Gather raw audio features\n",
    "        input_feats = [feat[\"input_features\"] for feat in features]\n",
    "        # 2. Pad them (this returns both 'input_features' and 'attention_mask')\n",
    "        batch_inputs = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": input_feats},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # 3. Gather label sequences\n",
    "        label_ids = [feat[\"labels\"] for feat in features]\n",
    "        # 4. Pad them (this returns 'input_ids' and its 'attention_mask')\n",
    "        label_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": label_ids},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # 5. Replace pad token ids in labels with -100 so they're ignored in loss\n",
    "        labels = label_batch[\"input_ids\"].masked_fill(\n",
    "            label_batch[\"attention_mask\"].ne(1), -100\n",
    "        )\n",
    "\n",
    "        # 6. If a bos token was prepended earlier, drop it here\n",
    "        if labels.size(1) > 0 and torch.all(labels[:, 0] == self.decoder_start_token_id):\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # 7. Package everything up\n",
    "        batch_inputs[\"labels\"] = labels\n",
    "        # and give the decoder its own attention mask\n",
    "        batch_inputs[\"decoder_attention_mask\"] = label_batch[\"attention_mask\"]\n",
    "\n",
    "        return batch_inputs\n",
    "        \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ab13a-b3ad-4cc4-9e98-22e468866e70",
   "metadata": {},
   "source": [
    "## 6. Metric Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d0c57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7805ca6c0e44f878bf36dc8fea40ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    pred_ids  = eval_pred.predictions            # already (batch, seq_len)\n",
    "    label_ids = eval_pred.label_ids\n",
    "\n",
    "    # Replace -100 so we can decode the references\n",
    "    label_ids = np.where(\n",
    "        label_ids != -100,\n",
    "        label_ids,\n",
    "        processor.tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    pred_str  = processor.batch_decode(pred_ids,  skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)*100\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244376e6-f8ed-4311-a96a-7f314d481941",
   "metadata": {},
   "source": [
    "## 7. Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b102d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir               = PUSH_MODEL_ID,\n",
    "\n",
    "    # --- Core Performance Optimizations ---\n",
    "    per_device_train_batch_size   = 8,\n",
    "    gradient_accumulation_steps   = 1,\n",
    "    per_device_eval_batch_size    = 4,\n",
    "    bf16                          = True,\n",
    "    fp16                          = False,\n",
    "    gradient_checkpointing        = False,\n",
    "\n",
    "    # --- Learning Schedule ---\n",
    "    learning_rate            = 2e-5,\n",
    "    warmup_steps             = 100,\n",
    "    max_steps                = 1500,\n",
    "    lr_scheduler_type        = \"cosine\",\n",
    "\n",
    "    # --- Logging and Saving ---\n",
    "    eval_strategy            = \"steps\",\n",
    "    eval_steps               = 300,\n",
    "    logging_steps            = 100,\n",
    "\n",
    "    # Turn off automatic checkpointing:\n",
    "    save_strategy            = \"no\",        # ← disable all intermediate saves\n",
    "    save_steps               = None,        # ← ignored when save_strategy=\"no\"\n",
    "    save_total_limit         = None,\n",
    "\n",
    "    # We’ll load the final model manually if you like, so disable this:\n",
    "    load_best_model_at_end   = False,\n",
    "\n",
    "    # --- Generation & Hub Push ---\n",
    "    predict_with_generate    = True,\n",
    "    generation_max_length    = 225,\n",
    "\n",
    "    report_to                = [\"tensorboard\"],\n",
    "\n",
    "    push_to_hub              = True,\n",
    "    hub_private_repo         = False,\n",
    "    hub_strategy             = \"end\",       # ← only push once after training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6be4ebb6-7d17-4d6e-8969-c2338ff5ccd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 38:58, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.624419</td>\n",
       "      <td>44.977595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.588100</td>\n",
       "      <td>0.508855</td>\n",
       "      <td>37.621359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.466200</td>\n",
       "      <td>0.434867</td>\n",
       "      <td>32.132188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.366100</td>\n",
       "      <td>0.363361</td>\n",
       "      <td>26.568335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.353437</td>\n",
       "      <td>25.784167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.4755880928039551, metrics={'train_runtime': 2339.7763, 'train_samples_per_second': 5.129, 'train_steps_per_second': 0.641, 'total_flos': 2.044747917361152e+19, 'train_loss': 0.4755880928039551, 'epoch': 1.272264631043257})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = dataset[\"train\"],\n",
    "    eval_dataset    = dataset[\"validation\"],\n",
    "    data_collator   = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca914e3c-7256-4ff8-a6e6-47fb4f8e6a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul  5 13:09:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             64W /  400W |   33809MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36af113-0960-4c75-be1f-3d342abca428",
   "metadata": {},
   "source": [
    "## 8. Save & Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67b05a2d-1778-4e9a-bed4-90e386d83628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdb5387f8594e97967a92ceb4e4d61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/3.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kingabzpro/whisper-large-v3-turbo-urdu/commit/bef7b66cc8393e9330613c53c60e656ac62d8fee', commit_message='Upload processor', commit_description='', oid='bef7b66cc8393e9330613c53c60e656ac62d8fee', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kingabzpro/whisper-large-v3-turbo-urdu', endpoint='https://huggingface.co', repo_type='model', repo_id='kingabzpro/whisper-large-v3-turbo-urdu'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()\n",
    "processor.save_pretrained(PUSH_MODEL_ID)\n",
    "processor.push_to_hub(PUSH_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b81d623c-31b9-45fe-b7d8-d3f9adb55a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133/2049976473.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(feat).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ہاروی وائنسٹن کے خلاف دوسری خاتون بھی جیری کے سامنے پیش\n"
     ]
    }
   ],
   "source": [
    "# grab your single example\n",
    "feat = dataset[\"validation\"][30][\"input_features\"]\n",
    "\n",
    "# turn it into a tensor and unsqueeze\n",
    "input_tensor = torch.tensor(feat).unsqueeze(0)\n",
    "\n",
    "# move to device AND cast to the model’s dtype\n",
    "model_dtype = next(model.parameters()).dtype\n",
    "input_tensor = input_tensor.to(device=model.device, dtype=model_dtype)\n",
    "\n",
    "# now generate\n",
    "pred_ids = model.generate(input_tensor)[0]\n",
    "print(\"Prediction:\", processor.decode(pred_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad8ea6c-5776-47df-9129-2754f1716956",
   "metadata": {},
   "source": [
    "## 9. Loading and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "835201b5-5680-4001-ae4d-972a59930696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original  : اگر عمران خان ٹھیک کر رہے ہیں۔\n",
      "Predicted : اگر عمران خان ٹھیک کر رہے ہیں۔\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch, warnings, os\n",
    "\n",
    "device = \"cuda:0\"\n",
    "torch_dtype = torch.float16\n",
    "model_id = \"kingabzpro/whisper-large-v3-turbo-urdu\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    ").to(device)\n",
    "model.config.use_cache = False\n",
    "model.generation_config.language = \"ur\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_17_0\",\n",
    "    \"ur\",\n",
    "    split=\"test\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"./hf_cache\",\n",
    ")\n",
    "audio = ds[100][\"audio\"]\n",
    "\n",
    "result = pipe(audio)\n",
    "print(\"Original  :\", ds[100][\"sentence\"])\n",
    "print(\"Predicted :\", result[\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
