{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16db3167",
   "metadata": {},
   "source": [
    "\n",
    "# Fine‑tune Whisper Large‑v3 Turbo on Urdu Common Voice 17.0  \n",
    "\n",
    "End-to-end fine-tuning of Whisper-v3-turbo on Urdu with TensorBoard experiment tracking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98441ef-f21d-4d62-8ebd-25e8a3aacc2e",
   "metadata": {},
   "source": [
    "## 0. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b8383b-5d3c-49cd-be5b-4f7d198f0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch torchaudio \"transformers==4.52.2\" accelerate \"datasets==3.4.1\" evaluate peft librosa bitsandbytes \\\n",
    "             huggingface_hub tensorboard jiwer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cea9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from huggingface_hub import login, HfApi\n",
    "from datasets import load_dataset, load_from_disk, Audio\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DefaultFlowCallback,\n",
    ")\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load as load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdacee-4b63-4a9d-81ab-3acbda1e09c1",
   "metadata": {},
   "source": [
    "## 1. Configuration & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872c6be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa339af69b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED         = 420\n",
    "HF_TOKEN     = os.environ.get(\"HF_TOKEN\")           # your HF token\n",
    "HF_USERNAME  = \"kingabzpro\"                         # replace as needed\n",
    "MODEL_ID     = \"openai/whisper-large-v3\"\n",
    "LANG_ID      = \"ur\"\n",
    "SAMPLING_RATE= 16_000\n",
    "CACHE_DIR    = \"./cached_cv_urdu\"\n",
    "PUSH_MODEL_ID = f\"{HF_USERNAME}/whisper-large-v3-urdu\"\n",
    "\n",
    "assert HF_TOKEN, \"Please set HF_TOKEN env var to your Hugging Face token\"\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca7345-8473-49e5-848f-d29634f4d2af",
   "metadata": {},
   "source": [
    "## 2. Helpers: text & audio cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b125b1-bbc4-4c56-92a8-305b8712b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_urdu(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[“”«»„”…—–\\[\\]\\(\\)]', '', text)\n",
    "    digits_map = str.maketrans(\"۰۱۲۳۴۵۶۷۸۹\", \"0123456789\")\n",
    "    return text.translate(digits_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d34b3-1fc6-467b-895c-86cc0997ea84",
   "metadata": {},
   "source": [
    "## 3. Login & Load Model + Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac72507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(HF_TOKEN)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID, language=LANG_ID, task=\"transcribe\"\n",
    ")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    MODEL_ID, ignore_mismatched_sizes=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.generation_config.language = \"ur\"\n",
    "model.generation_config.task = \"transcribe\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba73e15-de69-4c2d-9b0b-91b11a13231b",
   "metadata": {},
   "source": [
    "## 4. Preprocess + Cache Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0efd27ac-2976-460e-acaf-3afc6b0ccd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(batch):\n",
    "    # batch[\"audio\"] is a list of dictionaries because batched=True.\n",
    "    # We create a list of all the audio arrays from the batch.\n",
    "    audio_arrays = [x[\"array\"] for x in batch[\"audio\"]]\n",
    "    \n",
    "    # We can assume the sampling rate is the same for all items in the batch.\n",
    "    sampling_rate = batch[\"audio\"][0][\"sampling_rate\"]\n",
    "\n",
    "    input_feats = processor.feature_extractor(\n",
    "        audio_arrays, sampling_rate=sampling_rate, return_tensors=\"np\"\n",
    "    ).input_features\n",
    "\n",
    "    cleaned = [normalize_urdu(t) for t in batch[\"sentence\"]]\n",
    "    labels  = processor.tokenizer(\n",
    "        cleaned,\n",
    "        truncation=True,\n",
    "        max_length=model.config.max_length, # Uncomment if 'model' is defined\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\"\n",
    "    ).input_ids\n",
    "\n",
    "    # The function should return a dictionary\n",
    "    return {\"input_features\": input_feats, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "551f8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, DatasetDict, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "if not os.path.isdir(CACHE_DIR):\n",
    "    # 1. load both splits into a DatasetDict\n",
    "    dataset = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_17_0\",\n",
    "        LANG_ID,\n",
    "        split={\"train\": \"train+validation\", \"validation\": \"test[:600]\"},\n",
    "        cache_dir=\"./hf_cache\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # 2. drop unwanted columns\n",
    "    dataset = dataset.remove_columns(\n",
    "        [col for col in dataset[\"train\"].column_names if col not in (\"audio\",\"sentence\")]\n",
    "    )\n",
    "    print(dataset)\n",
    "    \n",
    "    # 3. \n",
    "    # Cast audio & preprocess\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
    "    dataset = dataset.map(\n",
    "        prepare_example,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        desc=\"Pre‑processing\",\n",
    "        batched=True,\n",
    "        batch_size=125,\n",
    "        load_from_cache_file=True,\n",
    "    )\n",
    "\n",
    "    # 4. save all splits\n",
    "    DatasetDict(dataset).save_to_disk(CACHE_DIR)\n",
    "\n",
    "# 5. load and set torch format\n",
    "dataset = load_from_disk(CACHE_DIR)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_features\",\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12587b8e-58dd-44c0-b6d7-26d7d48d02d0",
   "metadata": {},
   "source": [
    "## 5. Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0626a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # 1. Gather raw audio features\n",
    "        input_feats = [feat[\"input_features\"] for feat in features]\n",
    "        # 2. Pad them (this returns both 'input_features' and 'attention_mask')\n",
    "        batch_inputs = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": input_feats},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # 3. Gather label sequences\n",
    "        label_ids = [feat[\"labels\"] for feat in features]\n",
    "        # 4. Pad them (this returns 'input_ids' and its 'attention_mask')\n",
    "        label_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": label_ids},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # 5. Replace pad token ids in labels with -100 so they're ignored in loss\n",
    "        labels = label_batch[\"input_ids\"].masked_fill(\n",
    "            label_batch[\"attention_mask\"].ne(1), -100\n",
    "        )\n",
    "\n",
    "        # 6. If a bos token was prepended earlier, drop it here\n",
    "        if labels.size(1) > 0 and torch.all(labels[:, 0] == self.decoder_start_token_id):\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # 7. Package everything up\n",
    "        batch_inputs[\"labels\"] = labels\n",
    "        # and give the decoder its own attention mask\n",
    "        batch_inputs[\"decoder_attention_mask\"] = label_batch[\"attention_mask\"]\n",
    "\n",
    "        return batch_inputs\n",
    "        \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ab13a-b3ad-4cc4-9e98-22e468866e70",
   "metadata": {},
   "source": [
    "## 6. Metric Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d0c57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.61kB [00:00, 5.90MB/s]\n"
     ]
    }
   ],
   "source": [
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    pred_ids  = eval_pred.predictions            # already (batch, seq_len)\n",
    "    label_ids = eval_pred.label_ids\n",
    "\n",
    "    # Replace -100 so we can decode the references\n",
    "    label_ids = np.where(\n",
    "        label_ids != -100,\n",
    "        label_ids,\n",
    "        processor.tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    pred_str  = processor.batch_decode(pred_ids,  skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)*100\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str) * 100\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244376e6-f8ed-4311-a96a-7f314d481941",
   "metadata": {},
   "source": [
    "## 7. Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b102d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir               = PUSH_MODEL_ID,\n",
    "\n",
    "    # --- Core Performance Optimizations ---\n",
    "    per_device_train_batch_size   = 8,\n",
    "    gradient_accumulation_steps   = 2,\n",
    "    per_device_eval_batch_size    = 4,\n",
    "    bf16                          = True,\n",
    "    fp16                          = False,\n",
    "    gradient_checkpointing        = False,\n",
    "\n",
    "    # --- Learning Schedule ---\n",
    "    learning_rate            = 3e-5,\n",
    "    warmup_steps             = 100,\n",
    "    max_steps                = 1500,\n",
    "    lr_scheduler_type        = \"cosine\",\n",
    "\n",
    "    # --- Logging and Saving ---\n",
    "    eval_strategy            = \"steps\",\n",
    "    eval_steps               = 300,\n",
    "    logging_steps            = 100,\n",
    "\n",
    "    # Turn off automatic checkpointing:\n",
    "    save_strategy            = \"no\",        # ← disable all intermediate saves\n",
    "    save_steps               = None,        # ← ignored when save_strategy=\"no\"\n",
    "    save_total_limit         = None,\n",
    "\n",
    "    # We’ll load the final model manually if you like, so disable this:\n",
    "    load_best_model_at_end   = False,\n",
    "\n",
    "    # --- Generation & Hub Push ---\n",
    "    predict_with_generate    = True,\n",
    "    generation_max_length    = 225,\n",
    "\n",
    "    report_to                = [\"tensorboard\"],\n",
    "\n",
    "    push_to_hub              = True,\n",
    "    hub_private_repo         = False,\n",
    "    hub_strategy             = \"end\",       # ← only push once after training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6be4ebb6-7d17-4d6e-8969-c2338ff5ccd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 37:34, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>30.022405</td>\n",
       "      <td>10.364577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>25.858850</td>\n",
       "      <td>8.578023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>24.215833</td>\n",
       "      <td>7.941189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.019480</td>\n",
       "      <td>21.303211</td>\n",
       "      <td>7.201778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.020351</td>\n",
       "      <td>21.471247</td>\n",
       "      <td>7.197504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.01930886177221934, metrics={'train_runtime': 2256.3553, 'train_samples_per_second': 10.637, 'train_steps_per_second': 0.665, 'total_flos': 8.14380344082432e+19, 'train_loss': 0.01930886177221934, 'epoch': 2.542832909245123})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = dataset[\"train\"],\n",
    "    eval_dataset    = dataset[\"validation\"],\n",
    "    data_collator   = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca914e3c-7256-4ff8-a6e6-47fb4f8e6a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 11 13:14:21 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:DB:00.0 Off |                    0 |\n",
      "| N/A   28C    P0            116W /  700W |   65448MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2358      C   /usr/bin/python3                                0MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36af113-0960-4c75-be1f-3d342abca428",
   "metadata": {},
   "source": [
    "## 8. Save & Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67b05a2d-1778-4e9a-bed4-90e386d83628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kingabzpro/whisper-large-v3-urdu/commit/369eeb93b84a69e864684cff06637cded0d61a08', commit_message='Upload processor', commit_description='', oid='369eeb93b84a69e864684cff06637cded0d61a08', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kingabzpro/whisper-large-v3-urdu', endpoint='https://huggingface.co', repo_type='model', repo_id='kingabzpro/whisper-large-v3-urdu'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()\n",
    "processor.save_pretrained(PUSH_MODEL_ID)\n",
    "processor.push_to_hub(PUSH_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b81d623c-31b9-45fe-b7d8-d3f9adb55a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2358/2049976473.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(feat).unsqueeze(0)\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ہاروی وائنسٹن کے خلاف دوسری خاتون بھی جیری کے سامنے پیش\n"
     ]
    }
   ],
   "source": [
    "# grab your single example\n",
    "feat = dataset[\"validation\"][30][\"input_features\"]\n",
    "\n",
    "# turn it into a tensor and unsqueeze\n",
    "input_tensor = torch.tensor(feat).unsqueeze(0)\n",
    "\n",
    "# move to device AND cast to the model’s dtype\n",
    "model_dtype = next(model.parameters()).dtype\n",
    "input_tensor = input_tensor.to(device=model.device, dtype=model_dtype)\n",
    "\n",
    "# now generate\n",
    "pred_ids = model.generate(input_tensor)[0]\n",
    "print(\"Prediction:\", processor.decode(pred_ids, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
